{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1196732,"sourceType":"datasetVersion","datasetId":681625}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install patchify\n!pip install segmentation-models==1.0.1\n!pip install tensorflow==2.9.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-20T14:45:07.760972Z","iopub.execute_input":"2024-06-20T14:45:07.761388Z","iopub.status.idle":"2024-06-20T14:46:39.580680Z","shell.execute_reply.started":"2024-06-20T14:45:07.761357Z","shell.execute_reply":"2024-06-20T14:46:39.579586Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting patchify\n  Downloading patchify-0.2.3-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from patchify) (1.26.4)\nDownloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\nInstalling collected packages: patchify\nSuccessfully installed patchify-0.2.3\nCollecting segmentation-models==1.0.1\n  Downloading segmentation_models-1.0.1-py3-none-any.whl.metadata (938 bytes)\nCollecting keras-applications<=1.0.8,>=1.0.7 (from segmentation-models==1.0.1)\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\nCollecting image-classifiers==1.0.0 (from segmentation-models==1.0.1)\n  Downloading image_classifiers-1.0.0-py3-none-any.whl.metadata (8.6 kB)\nCollecting efficientnet==1.0.0 (from segmentation-models==1.0.1)\n  Downloading efficientnet-1.0.0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from efficientnet==1.0.0->segmentation-models==1.0.1) (0.22.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (1.26.4)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (3.10.0)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.11.4)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (3.2.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2023.12.9)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (21.3)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (3.1.1)\nDownloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\nDownloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\nDownloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\nDownloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\nSuccessfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\nCollecting tensorflow==2.9.0\n  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (1.6.3)\nCollecting flatbuffers<2,>=1.12 (from tensorflow==2.9.0)\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\nCollecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.0)\n  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (1.59.3)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (3.10.0)\nCollecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n  Downloading keras-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.0)\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (16.0.6)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (21.3)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (1.16.0)\nCollecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.0)\n  Downloading tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (0.35.0)\nCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (4.9.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.0) (1.14.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.26.1)\nCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.5.2)\nCollecting protobuf>=3.9.2 (from tensorflow==2.9.0)\n  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.32.3)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\nCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.9.0) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\nDownloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nDownloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n  Attempting uninstall: keras\n    Found existing installation: keras 3.3.3\n    Uninstalling keras-3.3.3:\n      Successfully uninstalled keras-3.3.3\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 23.5.26\n    Uninstalling flatbuffers-23.5.26:\n      Successfully uninstalled flatbuffers-23.5.26\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.15.0\n    Uninstalling tensorflow-estimator-2.15.0:\n      Successfully uninstalled tensorflow-estimator-2.15.0\n  Attempting uninstall: tensorboard-data-server\n    Found existing installation: tensorboard-data-server 0.7.2\n    Uninstalling tensorboard-data-server-0.7.2:\n      Successfully uninstalled tensorboard-data-server-0.7.2\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: gast\n    Found existing installation: gast 0.5.4\n    Uninstalling gast-0.5.4:\n      Successfully uninstalled gast-0.5.4\n  Attempting uninstall: google-auth-oauthlib\n    Found existing installation: google-auth-oauthlib 1.2.0\n    Uninstalling google-auth-oauthlib-1.2.0:\n      Successfully uninstalled google-auth-oauthlib-1.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ncudf 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nonnx 1.16.1 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\ntensorboardx 2.6.2.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.9.0 which is incompatible.\ntensorflow-serving-api 2.14.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-serving-api 2.14.1 requires tensorflow<3,>=2.14.1, but you have tensorflow 2.9.0 which is incompatible.\ntensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.9.0 which is incompatible.\ntf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom patchify import patchify\nfrom PIL import Image\nimport segmentation_models as sm\nfrom tensorflow.keras.metrics import MeanIoU\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nscaler = MinMaxScaler()\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:46:39.582987Z","iopub.execute_input":"2024-06-20T14:46:39.583347Z","iopub.status.idle":"2024-06-20T14:46:45.310511Z","shell.execute_reply.started":"2024-06-20T14:46:39.583311Z","shell.execute_reply":"2024-06-20T14:46:45.309494Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Segmentation Models: using `keras` framework.\n","output_type":"stream"}]},{"cell_type":"code","source":"root_directory = '/kaggle/input/semantic-segmentation-of-aerial-imagery'\n\npatch_size = 256","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:46:45.311759Z","iopub.execute_input":"2024-06-20T14:46:45.312339Z","iopub.status.idle":"2024-06-20T14:46:45.316680Z","shell.execute_reply.started":"2024-06-20T14:46:45.312309Z","shell.execute_reply":"2024-06-20T14:46:45.315768Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"image_dataset = []  \nfor path, subdirs, files in os.walk(root_directory):\n    #print(path)  \n    dirname = path.split(os.path.sep)[-1]\n    if dirname == 'images':   #Find all 'images' directories\n        images = os.listdir(path)  #List of all image names in this subdirectory\n        for i, image_name in enumerate(images):  \n            if image_name.endswith(\".jpg\"):   #Only read jpg images...\n               \n                image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n                SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n                SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n                image = Image.fromarray(image)\n                image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n                #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n                image = np.array(image)             \n       \n                #Extract patches from each image\n                print(\"Now patchifying image:\", path+\"/\"+image_name)\n                patches_img = patchify(image, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n        \n                for i in range(patches_img.shape[0]):\n                    for j in range(patches_img.shape[1]):\n                        \n                        single_patch_img = patches_img[i,j,:,:]\n                        \n                        #Use minmaxscaler instead of just dividing by 255. \n                        single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape)\n                        \n                        #single_patch_img = (single_patch_img.astype('float32')) / 255. \n                        single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.                               \n                        image_dataset.append(single_patch_img)\n                        \nmask_dataset = []  \nfor path, subdirs, files in os.walk(root_directory):\n    #print(path)  \n    dirname = path.split(os.path.sep)[-1]\n    if dirname == 'masks':   #Find all 'images' directories\n        masks = os.listdir(path)  #List of all image names in this subdirectory\n        for i, mask_name in enumerate(masks):  \n            if mask_name.endswith(\".png\"):   #Only read png images... (masks in this dataset)\n               \n                mask = cv2.imread(path+\"/\"+mask_name, 1)  #Read each image as Grey (or color but remember to map each color to an integer)\n                mask = cv2.cvtColor(mask,cv2.COLOR_BGR2RGB)\n                SIZE_X = (mask.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n                SIZE_Y = (mask.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n                mask = Image.fromarray(mask)\n                mask = mask.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n                #mask = mask.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n                mask = np.array(mask)             \n       \n                #Extract patches from each image\n                print(\"Now patchifying mask:\", path+\"/\"+mask_name)\n                patches_mask = patchify(mask, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n        \n                for i in range(patches_mask.shape[0]):\n                    for j in range(patches_mask.shape[1]):\n                        \n                        single_patch_mask = patches_mask[i,j,:,:]\n                        #single_patch_img = (single_patch_img.astype('float32')) / 255. #No need to scale masks, but you can do it if you want\n                        single_patch_mask = single_patch_mask[0] #Drop the extra unecessary dimension that patchify adds.                               \n                        mask_dataset.append(single_patch_mask) ","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:46:45.319729Z","iopub.execute_input":"2024-06-20T14:46:45.320263Z","iopub.status.idle":"2024-06-20T14:47:05.389788Z","shell.execute_reply.started":"2024-06-20T14:46:45.320227Z","shell.execute_reply":"2024-06-20T14:47:05.388720Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Now patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/images/image_part_002.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/images/image_part_006.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/images/image_part_005.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/images/image_part_003.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/images/image_part_004.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/images/image_part_007.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/images/image_part_009.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/images/image_part_008.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/images/image_part_001.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/images/image_part_002.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/images/image_part_006.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/images/image_part_005.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/images/image_part_003.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/images/image_part_004.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/images/image_part_007.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/images/image_part_009.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/images/image_part_008.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/images/image_part_001.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/images/image_part_002.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/images/image_part_006.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/images/image_part_005.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/images/image_part_003.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/images/image_part_004.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/images/image_part_007.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/images/image_part_009.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/images/image_part_008.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/images/image_part_001.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/images/image_part_002.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/images/image_part_006.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/images/image_part_005.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/images/image_part_003.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/images/image_part_004.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/images/image_part_007.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/images/image_part_009.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/images/image_part_008.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/images/image_part_001.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/images/image_part_002.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/images/image_part_006.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/images/image_part_005.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/images/image_part_003.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/images/image_part_004.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/images/image_part_007.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/images/image_part_009.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/images/image_part_008.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/images/image_part_001.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/images/image_part_002.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/images/image_part_006.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/images/image_part_005.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/images/image_part_003.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/images/image_part_004.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/images/image_part_007.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/images/image_part_009.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/images/image_part_008.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/images/image_part_001.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/images/image_part_002.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/images/image_part_006.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/images/image_part_005.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/images/image_part_003.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/images/image_part_004.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/images/image_part_007.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/images/image_part_009.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/images/image_part_008.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/images/image_part_001.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/images/image_part_002.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/images/image_part_006.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/images/image_part_005.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/images/image_part_003.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/images/image_part_004.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/images/image_part_007.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/images/image_part_009.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/images/image_part_008.jpg\nNow patchifying image: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/images/image_part_001.jpg\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/masks/image_part_001.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/masks/image_part_003.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/masks/image_part_006.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/masks/image_part_002.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/masks/image_part_008.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/masks/image_part_007.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/masks/image_part_009.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/masks/image_part_005.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 7/masks/image_part_004.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/masks/image_part_001.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/masks/image_part_003.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/masks/image_part_006.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/masks/image_part_002.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/masks/image_part_008.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/masks/image_part_007.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/masks/image_part_009.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/masks/image_part_005.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 8/masks/image_part_004.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/masks/image_part_001.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/masks/image_part_003.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/masks/image_part_006.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/masks/image_part_002.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/masks/image_part_008.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/masks/image_part_007.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/masks/image_part_009.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/masks/image_part_005.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 2/masks/image_part_004.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/masks/image_part_001.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/masks/image_part_003.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/masks/image_part_006.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/masks/image_part_002.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/masks/image_part_008.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/masks/image_part_007.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/masks/image_part_009.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/masks/image_part_005.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 5/masks/image_part_004.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/masks/image_part_001.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/masks/image_part_003.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/masks/image_part_006.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/masks/image_part_002.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/masks/image_part_008.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/masks/image_part_007.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/masks/image_part_009.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/masks/image_part_005.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 1/masks/image_part_004.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/masks/image_part_001.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/masks/image_part_003.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/masks/image_part_006.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/masks/image_part_002.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/masks/image_part_008.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/masks/image_part_007.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/masks/image_part_009.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/masks/image_part_005.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 3/masks/image_part_004.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/masks/image_part_001.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/masks/image_part_003.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/masks/image_part_006.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/masks/image_part_002.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/masks/image_part_008.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/masks/image_part_007.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/masks/image_part_009.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/masks/image_part_005.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/masks/image_part_004.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/masks/image_part_001.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/masks/image_part_003.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/masks/image_part_006.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/masks/image_part_002.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/masks/image_part_008.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/masks/image_part_007.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/masks/image_part_009.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/masks/image_part_005.png\nNow patchifying mask: /kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 4/masks/image_part_004.png\n","output_type":"stream"}]},{"cell_type":"code","source":"image_dataset = np.array(image_dataset)\nmask_dataset =  np.array(mask_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:47:05.391274Z","iopub.execute_input":"2024-06-20T14:47:05.391719Z","iopub.status.idle":"2024-06-20T14:47:06.062870Z","shell.execute_reply.started":"2024-06-20T14:47:05.391678Z","shell.execute_reply":"2024-06-20T14:47:06.061851Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Convert HEX to RGB array\na=int('3C', 16)  #3C with base 16. Should return 60. \nprint(a)\n\nBuilding = '#3C1098'.lstrip('#')\nBuilding = np.array(tuple(int(Building[i:i+2], 16) for i in (0, 2, 4))) # 60, 16, 152\n\nLand = '#8429F6'.lstrip('#')\nLand = np.array(tuple(int(Land[i:i+2], 16) for i in (0, 2, 4))) #132, 41, 246\n\nRoad = '#6EC1E4'.lstrip('#') \nRoad = np.array(tuple(int(Road[i:i+2], 16) for i in (0, 2, 4))) #110, 193, 228\n\nVegetation =  'FEDD3A'.lstrip('#') \nVegetation = np.array(tuple(int(Vegetation[i:i+2], 16) for i in (0, 2, 4))) #254, 221, 58\n\nWater = 'E2A929'.lstrip('#') \nWater = np.array(tuple(int(Water[i:i+2], 16) for i in (0, 2, 4))) #226, 169, 41\n\nUnlabeled = '#9B9B9B'.lstrip('#') \nUnlabeled = np.array(tuple(int(Unlabeled[i:i+2], 16) for i in (0, 2, 4))) #155, 155, 155\n\nlabel = single_patch_mask","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:47:06.064100Z","iopub.execute_input":"2024-06-20T14:47:06.064420Z","iopub.status.idle":"2024-06-20T14:47:06.075307Z","shell.execute_reply.started":"2024-06-20T14:47:06.064394Z","shell.execute_reply":"2024-06-20T14:47:06.074164Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"60\n","output_type":"stream"}]},{"cell_type":"code","source":"def rgb_to_2D_label(label):\n    \"\"\"\n    Suply our labale masks as input in RGB format. \n    Replace pixels with specific RGB values ...\n    \"\"\"\n    label_seg = np.zeros(label.shape,dtype=np.uint8)\n    label_seg [np.all(label == Building,axis=-1)] = 0\n    label_seg [np.all(label==Land,axis=-1)] = 1\n    label_seg [np.all(label==Road,axis=-1)] = 2\n    label_seg [np.all(label==Vegetation,axis=-1)] = 3\n    label_seg [np.all(label==Water,axis=-1)] = 4\n    label_seg [np.all(label==Unlabeled,axis=-1)] = 5\n    \n    label_seg = label_seg[:,:,0]  #Just take the first channel, no need for all 3 channels\n    \n    return label_seg","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:47:06.076917Z","iopub.execute_input":"2024-06-20T14:47:06.077327Z","iopub.status.idle":"2024-06-20T14:47:06.087230Z","shell.execute_reply.started":"2024-06-20T14:47:06.077290Z","shell.execute_reply":"2024-06-20T14:47:06.086359Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"labels = []\nfor i in range(mask_dataset.shape[0]):\n    label = rgb_to_2D_label(mask_dataset[i])\n    labels.append(label)    \n\nlabels = np.array(labels)   \nlabels = np.expand_dims(labels, axis=3)\n \n\nprint(\"Unique labels in label dataset are: \", np.unique(labels))","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:47:06.088428Z","iopub.execute_input":"2024-06-20T14:47:06.088769Z","iopub.status.idle":"2024-06-20T14:47:30.448232Z","shell.execute_reply.started":"2024-06-20T14:47:06.088700Z","shell.execute_reply":"2024-06-20T14:47:30.447213Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Unique labels in label dataset are:  [0 1 2 3 4 5]\n","output_type":"stream"}]},{"cell_type":"code","source":"n_classes = len(np.unique(labels))\nfrom keras.utils import to_categorical\nlabels_cat = to_categorical(labels, num_classes=n_classes)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(image_dataset, labels_cat, test_size = 0.20, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:47:30.449641Z","iopub.execute_input":"2024-06-20T14:47:30.450001Z","iopub.status.idle":"2024-06-20T14:47:35.856794Z","shell.execute_reply.started":"2024-06-20T14:47:30.449969Z","shell.execute_reply":"2024-06-20T14:47:35.856008Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\nfrom keras import backend as K\n\ndef jacard_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n\ndef multi_unet_model(n_classes=4, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=1):\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n    s = inputs\n\n    #Contraction path\n    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n    c1 = Dropout(0.2)(c1)  # Original 0.1\n    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n    p1 = MaxPooling2D((2, 2))(c1)\n    \n    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n    c2 = Dropout(0.2)(c2)  # Original 0.1\n    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n    p2 = MaxPooling2D((2, 2))(c2)\n     \n    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n    c3 = Dropout(0.2)(c3)\n    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n    p3 = MaxPooling2D((2, 2))(c3)\n     \n    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n    c4 = Dropout(0.2)(c4)\n    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n     \n    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n    c5 = Dropout(0.3)(c5)\n    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n    \n    #Expansive path \n    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n    c6 = Dropout(0.2)(c6)\n    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n     \n    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n    c7 = Dropout(0.2)(c7)\n    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n     \n    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n    c8 = Dropout(0.2)(c8)  # Original 0.1\n    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n     \n    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n    c9 = Dropout(0.2)(c9)  # Original 0.1\n    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n     \n    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n     \n    model = Model(inputs=[inputs], outputs=[outputs])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:47:35.859625Z","iopub.execute_input":"2024-06-20T14:47:35.859931Z","iopub.status.idle":"2024-06-20T14:47:35.882439Z","shell.execute_reply.started":"2024-06-20T14:47:35.859904Z","shell.execute_reply":"2024-06-20T14:47:35.881519Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Parameters for model\n# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n# set class weights for dice_loss\n\nweights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\ndice_loss = sm.losses.DiceLoss(class_weights=weights) \nfocal_loss = sm.losses.CategoricalFocalLoss()\ntotal_loss = dice_loss + (1 * focal_loss)  #\n\n\nIMG_HEIGHT = X_train.shape[1]\nIMG_WIDTH  = X_train.shape[2]\nIMG_CHANNELS = X_train.shape[3]","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:47:35.883627Z","iopub.execute_input":"2024-06-20T14:47:35.883962Z","iopub.status.idle":"2024-06-20T14:47:35.903435Z","shell.execute_reply.started":"2024-06-20T14:47:35.883927Z","shell.execute_reply":"2024-06-20T14:47:35.902591Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"metrics=['accuracy', jacard_coef]\n\ndef get_model():\n    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n\nmodel = get_model()\nmodel.compile(optimizer='adam', loss=total_loss, metrics=metrics)\n#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:47:35.904638Z","iopub.execute_input":"2024-06-20T14:47:35.904932Z","iopub.status.idle":"2024-06-20T14:47:36.772640Z","shell.execute_reply.started":"2024-06-20T14:47:35.904896Z","shell.execute_reply":"2024-06-20T14:47:36.771946Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n                                )]                                                                \n                                                                                                  \n conv2d (Conv2D)                (None, 256, 256, 16  448         ['input_1[0][0]']                \n                                )                                                                 \n                                                                                                  \n dropout (Dropout)              (None, 256, 256, 16  0           ['conv2d[0][0]']                 \n                                )                                                                 \n                                                                                                  \n conv2d_1 (Conv2D)              (None, 256, 256, 16  2320        ['dropout[0][0]']                \n                                )                                                                 \n                                                                                                  \n max_pooling2d (MaxPooling2D)   (None, 128, 128, 16  0           ['conv2d_1[0][0]']               \n                                )                                                                 \n                                                                                                  \n conv2d_2 (Conv2D)              (None, 128, 128, 32  4640        ['max_pooling2d[0][0]']          \n                                )                                                                 \n                                                                                                  \n dropout_1 (Dropout)            (None, 128, 128, 32  0           ['conv2d_2[0][0]']               \n                                )                                                                 \n                                                                                                  \n conv2d_3 (Conv2D)              (None, 128, 128, 32  9248        ['dropout_1[0][0]']              \n                                )                                                                 \n                                                                                                  \n max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)  0           ['conv2d_3[0][0]']               \n                                                                                                  \n conv2d_4 (Conv2D)              (None, 64, 64, 64)   18496       ['max_pooling2d_1[0][0]']        \n                                                                                                  \n dropout_2 (Dropout)            (None, 64, 64, 64)   0           ['conv2d_4[0][0]']               \n                                                                                                  \n conv2d_5 (Conv2D)              (None, 64, 64, 64)   36928       ['dropout_2[0][0]']              \n                                                                                                  \n max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)  0           ['conv2d_5[0][0]']               \n                                                                                                  \n conv2d_6 (Conv2D)              (None, 32, 32, 128)  73856       ['max_pooling2d_2[0][0]']        \n                                                                                                  \n dropout_3 (Dropout)            (None, 32, 32, 128)  0           ['conv2d_6[0][0]']               \n                                                                                                  \n conv2d_7 (Conv2D)              (None, 32, 32, 128)  147584      ['dropout_3[0][0]']              \n                                                                                                  \n max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0          ['conv2d_7[0][0]']               \n                                                                                                  \n conv2d_8 (Conv2D)              (None, 16, 16, 256)  295168      ['max_pooling2d_3[0][0]']        \n                                                                                                  \n dropout_4 (Dropout)            (None, 16, 16, 256)  0           ['conv2d_8[0][0]']               \n                                                                                                  \n conv2d_9 (Conv2D)              (None, 16, 16, 256)  590080      ['dropout_4[0][0]']              \n                                                                                                  \n conv2d_transpose (Conv2DTransp  (None, 32, 32, 128)  131200     ['conv2d_9[0][0]']               \n ose)                                                                                             \n                                                                                                  \n concatenate (Concatenate)      (None, 32, 32, 256)  0           ['conv2d_transpose[0][0]',       \n                                                                  'conv2d_7[0][0]']               \n                                                                                                  \n conv2d_10 (Conv2D)             (None, 32, 32, 128)  295040      ['concatenate[0][0]']            \n                                                                                                  \n dropout_5 (Dropout)            (None, 32, 32, 128)  0           ['conv2d_10[0][0]']              \n                                                                                                  \n conv2d_11 (Conv2D)             (None, 32, 32, 128)  147584      ['dropout_5[0][0]']              \n                                                                                                  \n conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 64)  32832       ['conv2d_11[0][0]']              \n spose)                                                                                           \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 64, 64, 128)  0           ['conv2d_transpose_1[0][0]',     \n                                                                  'conv2d_5[0][0]']               \n                                                                                                  \n conv2d_12 (Conv2D)             (None, 64, 64, 64)   73792       ['concatenate_1[0][0]']          \n                                                                                                  \n dropout_6 (Dropout)            (None, 64, 64, 64)   0           ['conv2d_12[0][0]']              \n                                                                                                  \n conv2d_13 (Conv2D)             (None, 64, 64, 64)   36928       ['dropout_6[0][0]']              \n                                                                                                  \n conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 32  8224       ['conv2d_13[0][0]']              \n spose)                         )                                                                 \n                                                                                                  \n concatenate_2 (Concatenate)    (None, 128, 128, 64  0           ['conv2d_transpose_2[0][0]',     \n                                )                                 'conv2d_3[0][0]']               \n                                                                                                  \n conv2d_14 (Conv2D)             (None, 128, 128, 32  18464       ['concatenate_2[0][0]']          \n                                )                                                                 \n                                                                                                  \n dropout_7 (Dropout)            (None, 128, 128, 32  0           ['conv2d_14[0][0]']              \n                                )                                                                 \n                                                                                                  \n conv2d_15 (Conv2D)             (None, 128, 128, 32  9248        ['dropout_7[0][0]']              \n                                )                                                                 \n                                                                                                  \n conv2d_transpose_3 (Conv2DTran  (None, 256, 256, 16  2064       ['conv2d_15[0][0]']              \n spose)                         )                                                                 \n                                                                                                  \n concatenate_3 (Concatenate)    (None, 256, 256, 32  0           ['conv2d_transpose_3[0][0]',     \n                                )                                 'conv2d_1[0][0]']               \n                                                                                                  \n conv2d_16 (Conv2D)             (None, 256, 256, 16  4624        ['concatenate_3[0][0]']          \n                                )                                                                 \n                                                                                                  \n dropout_8 (Dropout)            (None, 256, 256, 16  0           ['conv2d_16[0][0]']              \n                                )                                                                 \n                                                                                                  \n conv2d_17 (Conv2D)             (None, 256, 256, 16  2320        ['dropout_8[0][0]']              \n                                )                                                                 \n                                                                                                  \n conv2d_18 (Conv2D)             (None, 256, 256, 6)  102         ['conv2d_17[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 1,941,190\nTrainable params: 1,941,190\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\n# Define a callback to save the model every 10 epochs\ncheckpoint_filepath = '/kaggle/working/models/model_checkpoint.h5'\ncheckpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_freq='epoch',  # Save every epoch\n    save_best_only=False,  # Save every model\n    monitor='val_loss',  # Monitor validation loss\n    mode='min',  # Save the model with minimum validation loss\n    verbose=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:47:36.777327Z","iopub.execute_input":"2024-06-20T14:47:36.777945Z","iopub.status.idle":"2024-06-20T14:47:36.783920Z","shell.execute_reply.started":"2024-06-20T14:47:36.777907Z","shell.execute_reply":"2024-06-20T14:47:36.782953Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    X_train, y_train, \n    batch_size=16, \n    epochs=100, \n    verbose=1, \n    validation_data=(X_test, y_test), \n    shuffle=False,\n    callbacks=[checkpoint_callback]\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:47:36.785167Z","iopub.execute_input":"2024-06-20T14:47:36.785516Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n66/66 [==============================] - ETA: 0s - loss: 1.0192 - accuracy: 0.4781 - jacard_coef: 0.1503\nEpoch 1: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 420s 6s/step - loss: 1.0192 - accuracy: 0.4781 - jacard_coef: 0.1503 - val_loss: 1.0144 - val_accuracy: 0.5090 - val_jacard_coef: 0.1574\nEpoch 2/100\n66/66 [==============================] - ETA: 0s - loss: 1.0117 - accuracy: 0.5387 - jacard_coef: 0.1765\nEpoch 2: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 410s 6s/step - loss: 1.0117 - accuracy: 0.5387 - jacard_coef: 0.1765 - val_loss: 1.0115 - val_accuracy: 0.5090 - val_jacard_coef: 0.1644\nEpoch 3/100\n66/66 [==============================] - ETA: 0s - loss: 1.0093 - accuracy: 0.5387 - jacard_coef: 0.1849\nEpoch 3: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 412s 6s/step - loss: 1.0093 - accuracy: 0.5387 - jacard_coef: 0.1849 - val_loss: 1.0090 - val_accuracy: 0.5091 - val_jacard_coef: 0.1719\nEpoch 4/100\n66/66 [==============================] - ETA: 0s - loss: 1.0071 - accuracy: 0.5495 - jacard_coef: 0.1937\nEpoch 4: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 405s 6s/step - loss: 1.0071 - accuracy: 0.5495 - jacard_coef: 0.1937 - val_loss: 1.0039 - val_accuracy: 0.5436 - val_jacard_coef: 0.1921\nEpoch 5/100\n66/66 [==============================] - ETA: 0s - loss: 1.0037 - accuracy: 0.5563 - jacard_coef: 0.2082\nEpoch 5: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 410s 6s/step - loss: 1.0037 - accuracy: 0.5563 - jacard_coef: 0.2082 - val_loss: 1.0011 - val_accuracy: 0.5457 - val_jacard_coef: 0.2026\nEpoch 6/100\n66/66 [==============================] - ETA: 0s - loss: 1.0025 - accuracy: 0.5531 - jacard_coef: 0.2109\nEpoch 6: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 408s 6s/step - loss: 1.0025 - accuracy: 0.5531 - jacard_coef: 0.2109 - val_loss: 1.0005 - val_accuracy: 0.5460 - val_jacard_coef: 0.2100\nEpoch 7/100\n66/66 [==============================] - ETA: 0s - loss: 1.0012 - accuracy: 0.5532 - jacard_coef: 0.2155\nEpoch 7: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 404s 6s/step - loss: 1.0012 - accuracy: 0.5532 - jacard_coef: 0.2155 - val_loss: 0.9998 - val_accuracy: 0.5458 - val_jacard_coef: 0.2075\nEpoch 8/100\n66/66 [==============================] - ETA: 0s - loss: 1.0006 - accuracy: 0.5515 - jacard_coef: 0.2162\nEpoch 8: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 399s 6s/step - loss: 1.0006 - accuracy: 0.5515 - jacard_coef: 0.2162 - val_loss: 0.9991 - val_accuracy: 0.5450 - val_jacard_coef: 0.2100\nEpoch 9/100\n66/66 [==============================] - ETA: 0s - loss: 1.0003 - accuracy: 0.5520 - jacard_coef: 0.2170\nEpoch 9: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 401s 6s/step - loss: 1.0003 - accuracy: 0.5520 - jacard_coef: 0.2170 - val_loss: 0.9990 - val_accuracy: 0.5454 - val_jacard_coef: 0.2097\nEpoch 10/100\n66/66 [==============================] - ETA: 0s - loss: 0.9999 - accuracy: 0.5532 - jacard_coef: 0.2176\nEpoch 10: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 401s 6s/step - loss: 0.9999 - accuracy: 0.5532 - jacard_coef: 0.2176 - val_loss: 0.9986 - val_accuracy: 0.5456 - val_jacard_coef: 0.2102\nEpoch 11/100\n66/66 [==============================] - ETA: 0s - loss: 0.9997 - accuracy: 0.5533 - jacard_coef: 0.2178\nEpoch 11: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 401s 6s/step - loss: 0.9997 - accuracy: 0.5533 - jacard_coef: 0.2178 - val_loss: 0.9989 - val_accuracy: 0.5455 - val_jacard_coef: 0.2090\nEpoch 12/100\n66/66 [==============================] - ETA: 0s - loss: 0.9995 - accuracy: 0.5558 - jacard_coef: 0.2185\nEpoch 12: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 388s 6s/step - loss: 0.9995 - accuracy: 0.5558 - jacard_coef: 0.2185 - val_loss: 0.9986 - val_accuracy: 0.5456 - val_jacard_coef: 0.2102\nEpoch 13/100\n66/66 [==============================] - ETA: 0s - loss: 0.9992 - accuracy: 0.5558 - jacard_coef: 0.2189\nEpoch 13: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 386s 6s/step - loss: 0.9992 - accuracy: 0.5558 - jacard_coef: 0.2189 - val_loss: 0.9985 - val_accuracy: 0.5445 - val_jacard_coef: 0.2101\nEpoch 14/100\n66/66 [==============================] - ETA: 0s - loss: 0.9991 - accuracy: 0.5575 - jacard_coef: 0.2195\nEpoch 14: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 395s 6s/step - loss: 0.9991 - accuracy: 0.5575 - jacard_coef: 0.2195 - val_loss: 0.9988 - val_accuracy: 0.5433 - val_jacard_coef: 0.2095\nEpoch 15/100\n66/66 [==============================] - ETA: 0s - loss: 0.9989 - accuracy: 0.5593 - jacard_coef: 0.2198\nEpoch 15: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 398s 6s/step - loss: 0.9989 - accuracy: 0.5593 - jacard_coef: 0.2198 - val_loss: 0.9987 - val_accuracy: 0.5414 - val_jacard_coef: 0.2120\nEpoch 16/100\n66/66 [==============================] - ETA: 0s - loss: 0.9986 - accuracy: 0.5585 - jacard_coef: 0.2209\nEpoch 16: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 396s 6s/step - loss: 0.9986 - accuracy: 0.5585 - jacard_coef: 0.2209 - val_loss: 0.9983 - val_accuracy: 0.5413 - val_jacard_coef: 0.2116\nEpoch 17/100\n66/66 [==============================] - ETA: 0s - loss: 0.9985 - accuracy: 0.5593 - jacard_coef: 0.2211\nEpoch 17: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 397s 6s/step - loss: 0.9985 - accuracy: 0.5593 - jacard_coef: 0.2211 - val_loss: 0.9981 - val_accuracy: 0.5420 - val_jacard_coef: 0.2108\nEpoch 18/100\n66/66 [==============================] - ETA: 0s - loss: 0.9983 - accuracy: 0.5611 - jacard_coef: 0.2218\nEpoch 18: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 393s 6s/step - loss: 0.9983 - accuracy: 0.5611 - jacard_coef: 0.2218 - val_loss: 0.9979 - val_accuracy: 0.5406 - val_jacard_coef: 0.2131\nEpoch 19/100\n66/66 [==============================] - ETA: 0s - loss: 0.9981 - accuracy: 0.5613 - jacard_coef: 0.2229\nEpoch 19: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 408s 6s/step - loss: 0.9981 - accuracy: 0.5613 - jacard_coef: 0.2229 - val_loss: 0.9982 - val_accuracy: 0.5417 - val_jacard_coef: 0.2112\nEpoch 20/100\n66/66 [==============================] - ETA: 0s - loss: 0.9978 - accuracy: 0.5633 - jacard_coef: 0.2233\nEpoch 20: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 410s 6s/step - loss: 0.9978 - accuracy: 0.5633 - jacard_coef: 0.2233 - val_loss: 0.9978 - val_accuracy: 0.5419 - val_jacard_coef: 0.2121\nEpoch 21/100\n66/66 [==============================] - ETA: 0s - loss: 0.9976 - accuracy: 0.5633 - jacard_coef: 0.2237\nEpoch 21: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 408s 6s/step - loss: 0.9976 - accuracy: 0.5633 - jacard_coef: 0.2237 - val_loss: 0.9976 - val_accuracy: 0.5430 - val_jacard_coef: 0.2109\nEpoch 22/100\n66/66 [==============================] - ETA: 0s - loss: 0.9974 - accuracy: 0.5650 - jacard_coef: 0.2245\nEpoch 22: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 409s 6s/step - loss: 0.9974 - accuracy: 0.5650 - jacard_coef: 0.2245 - val_loss: 0.9974 - val_accuracy: 0.5430 - val_jacard_coef: 0.2110\nEpoch 23/100\n66/66 [==============================] - ETA: 0s - loss: 0.9974 - accuracy: 0.5642 - jacard_coef: 0.2250\nEpoch 23: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 416s 6s/step - loss: 0.9974 - accuracy: 0.5642 - jacard_coef: 0.2250 - val_loss: 0.9974 - val_accuracy: 0.5425 - val_jacard_coef: 0.2126\nEpoch 24/100\n66/66 [==============================] - ETA: 0s - loss: 0.9970 - accuracy: 0.5653 - jacard_coef: 0.2264\nEpoch 24: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 413s 6s/step - loss: 0.9970 - accuracy: 0.5653 - jacard_coef: 0.2264 - val_loss: 0.9971 - val_accuracy: 0.5413 - val_jacard_coef: 0.2115\nEpoch 25/100\n66/66 [==============================] - ETA: 0s - loss: 0.9968 - accuracy: 0.5638 - jacard_coef: 0.2264\nEpoch 25: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 415s 6s/step - loss: 0.9968 - accuracy: 0.5638 - jacard_coef: 0.2264 - val_loss: 0.9971 - val_accuracy: 0.5428 - val_jacard_coef: 0.2111\nEpoch 26/100\n66/66 [==============================] - ETA: 0s - loss: 0.9967 - accuracy: 0.5672 - jacard_coef: 0.2274\nEpoch 26: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 408s 6s/step - loss: 0.9967 - accuracy: 0.5672 - jacard_coef: 0.2274 - val_loss: 0.9972 - val_accuracy: 0.5398 - val_jacard_coef: 0.2098\nEpoch 27/100\n66/66 [==============================] - ETA: 0s - loss: 0.9965 - accuracy: 0.5670 - jacard_coef: 0.2282\nEpoch 27: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 402s 6s/step - loss: 0.9965 - accuracy: 0.5670 - jacard_coef: 0.2282 - val_loss: 0.9968 - val_accuracy: 0.5413 - val_jacard_coef: 0.2237\nEpoch 28/100\n66/66 [==============================] - ETA: 0s - loss: 0.9961 - accuracy: 0.5668 - jacard_coef: 0.2288\nEpoch 28: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 391s 6s/step - loss: 0.9961 - accuracy: 0.5668 - jacard_coef: 0.2288 - val_loss: 0.9962 - val_accuracy: 0.5442 - val_jacard_coef: 0.2257\nEpoch 29/100\n66/66 [==============================] - ETA: 0s - loss: 0.9961 - accuracy: 0.5678 - jacard_coef: 0.2296\nEpoch 29: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 395s 6s/step - loss: 0.9961 - accuracy: 0.5678 - jacard_coef: 0.2296 - val_loss: 0.9965 - val_accuracy: 0.5467 - val_jacard_coef: 0.2320\nEpoch 30/100\n66/66 [==============================] - ETA: 0s - loss: 0.9957 - accuracy: 0.5673 - jacard_coef: 0.2313\nEpoch 30: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 410s 6s/step - loss: 0.9957 - accuracy: 0.5673 - jacard_coef: 0.2313 - val_loss: 0.9966 - val_accuracy: 0.5426 - val_jacard_coef: 0.2137\nEpoch 31/100\n66/66 [==============================] - ETA: 0s - loss: 0.9955 - accuracy: 0.5681 - jacard_coef: 0.2315\nEpoch 31: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 413s 6s/step - loss: 0.9955 - accuracy: 0.5681 - jacard_coef: 0.2315 - val_loss: 0.9967 - val_accuracy: 0.5402 - val_jacard_coef: 0.2306\nEpoch 32/100\n66/66 [==============================] - ETA: 0s - loss: 0.9954 - accuracy: 0.5687 - jacard_coef: 0.2318\nEpoch 32: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 407s 6s/step - loss: 0.9954 - accuracy: 0.5687 - jacard_coef: 0.2318 - val_loss: 0.9962 - val_accuracy: 0.5427 - val_jacard_coef: 0.2279\nEpoch 33/100\n66/66 [==============================] - ETA: 0s - loss: 0.9955 - accuracy: 0.5707 - jacard_coef: 0.2316\nEpoch 33: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 398s 6s/step - loss: 0.9955 - accuracy: 0.5707 - jacard_coef: 0.2316 - val_loss: 0.9963 - val_accuracy: 0.5399 - val_jacard_coef: 0.2274\nEpoch 34/100\n66/66 [==============================] - ETA: 0s - loss: 0.9954 - accuracy: 0.5712 - jacard_coef: 0.2323\nEpoch 34: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 410s 6s/step - loss: 0.9954 - accuracy: 0.5712 - jacard_coef: 0.2323 - val_loss: 0.9975 - val_accuracy: 0.5400 - val_jacard_coef: 0.2051\nEpoch 35/100\n66/66 [==============================] - ETA: 0s - loss: 0.9956 - accuracy: 0.5708 - jacard_coef: 0.2305\nEpoch 35: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 421s 6s/step - loss: 0.9956 - accuracy: 0.5708 - jacard_coef: 0.2305 - val_loss: 0.9963 - val_accuracy: 0.5417 - val_jacard_coef: 0.2209\nEpoch 36/100\n66/66 [==============================] - ETA: 0s - loss: 0.9947 - accuracy: 0.5729 - jacard_coef: 0.2343\nEpoch 36: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 419s 6s/step - loss: 0.9947 - accuracy: 0.5729 - jacard_coef: 0.2343 - val_loss: 0.9960 - val_accuracy: 0.5429 - val_jacard_coef: 0.2308\nEpoch 37/100\n66/66 [==============================] - ETA: 0s - loss: 0.9947 - accuracy: 0.5713 - jacard_coef: 0.2347\nEpoch 37: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 418s 6s/step - loss: 0.9947 - accuracy: 0.5713 - jacard_coef: 0.2347 - val_loss: 0.9970 - val_accuracy: 0.5427 - val_jacard_coef: 0.2380\nEpoch 38/100\n66/66 [==============================] - ETA: 0s - loss: 0.9946 - accuracy: 0.5711 - jacard_coef: 0.2343\nEpoch 38: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 419s 6s/step - loss: 0.9946 - accuracy: 0.5711 - jacard_coef: 0.2343 - val_loss: 0.9960 - val_accuracy: 0.5399 - val_jacard_coef: 0.2302\nEpoch 39/100\n66/66 [==============================] - ETA: 0s - loss: 0.9942 - accuracy: 0.5742 - jacard_coef: 0.2370\nEpoch 39: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 414s 6s/step - loss: 0.9942 - accuracy: 0.5742 - jacard_coef: 0.2370 - val_loss: 0.9960 - val_accuracy: 0.5442 - val_jacard_coef: 0.2329\nEpoch 40/100\n66/66 [==============================] - ETA: 0s - loss: 0.9939 - accuracy: 0.5759 - jacard_coef: 0.2381\nEpoch 40: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 417s 6s/step - loss: 0.9939 - accuracy: 0.5759 - jacard_coef: 0.2381 - val_loss: 0.9956 - val_accuracy: 0.5397 - val_jacard_coef: 0.2325\nEpoch 41/100\n66/66 [==============================] - ETA: 0s - loss: 0.9937 - accuracy: 0.5743 - jacard_coef: 0.2389\nEpoch 41: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 406s 6s/step - loss: 0.9937 - accuracy: 0.5743 - jacard_coef: 0.2389 - val_loss: 0.9963 - val_accuracy: 0.5414 - val_jacard_coef: 0.2349\nEpoch 42/100\n66/66 [==============================] - ETA: 0s - loss: 0.9936 - accuracy: 0.5758 - jacard_coef: 0.2389\nEpoch 42: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 411s 6s/step - loss: 0.9936 - accuracy: 0.5758 - jacard_coef: 0.2389 - val_loss: 0.9956 - val_accuracy: 0.5412 - val_jacard_coef: 0.2311\nEpoch 43/100\n66/66 [==============================] - ETA: 0s - loss: 0.9941 - accuracy: 0.5721 - jacard_coef: 0.2379\nEpoch 43: saving model to /kaggle/working/models/model_checkpoint.h5\n66/66 [==============================] - 403s 6s/step - loss: 0.9941 - accuracy: 0.5721 - jacard_coef: 0.2379 - val_loss: 0.9961 - val_accuracy: 0.5435 - val_jacard_coef: 0.2350\nEpoch 44/100\n54/66 [=======================>......] - ETA: 1:07 - loss: 0.9937 - accuracy: 0.5754 - jacard_coef: 0.2391","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import load_model\n\n# Path to your saved checkpoint\ncheckpoint_filepath = '/kaggle/working/models/model_checkpoint.h5'\n\n# Load the saved model\nmodel = load_model(checkpoint_filepath, custom_objects={'dice_loss_plus_2focal_loss': total_loss, 'jacard_coef': jacard_coef})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the training and validation accuracy and loss at each epoch\nhistory = history1\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'y', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc = history.history['jacard_coef']\nval_acc = history.history['val_jacard_coef']\n\nplt.plot(epochs, acc, 'y', label='Training IoU')\nplt.plot(epochs, val_acc, 'r', label='Validation IoU')\nplt.title('Training and validation IoU')\nplt.xlabel('Epochs')\nplt.ylabel('IoU')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import save_model\n\n# Replace 'model' with your trained model variable\nmodel_path = '/kaggle/working/models/model.h5'\nsave_model(model, model_path)\nprint(f\"Model saved successfully at {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}